{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après plusieurs entretiens qui ont pour la majorité abouti à une mise en suspension du processsus de recrutement avec comme argument principale : \n",
    "\"Nous sommes très intéressés par votre profil mais malheureusement nous avons peu de missions pour le moment, nous vous recontactons dès que nous avons quelcque chose et de votre côté tenez-nous au courant si vous avez des propositions.\"\n",
    "\n",
    "Ayant atteint mon objectif personnel de renforcer mes compétences en python à un niveau proche de celui que j'ai en R (7 ans d'expérience) grâce notamment aux deux livres suivants:\n",
    "\n",
    "1. Apprendre à programmer en python\n",
    "2. Machine learning avec python\n",
    "\n",
    "En attendant deux autres retours d'entretiens, j'ai acquis un troisième livre (Deep Learning avec TensorFlow) et je m'apprêtais poursuivre mon apprentissage du deep learning initié il y a plus d'un an à travers la spécialisation deep learning de *deepleraning.ai* sur coursera lorsque je me suis dit que je voulais aller plus loin que la simple lecture du livre et la résolution des exercices proposés.\n",
    "\n",
    "Après quelques minutes de réflexion j'ai assez rapidement trouvé un sujet qui m'occuperait vraiment l'esprit et qui me ferait grandement progresser dans mon métier de data scientist. J'ai décidé de construire un modèle de machine translation pour traduire mon rapport de stage de fin d'étude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = helper.load_data('small_vocab_en')\n",
    "french_sentences = helper.load_data('small_vocab_fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\"\",\"\".\"\"in\"\"it\"\"during\"\"the\"\"but\"\"and\"\"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\"\".\"\",\"\"en\"\"il\"\"les\"\"mais\"\"et\"\"la\"\"parfois\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\"\"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\"\"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input: The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input: By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input: This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    x_tk = Tokenizer(char_level=False)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "text_sentences = ['The quick brown fox jumps over the lazy dog .',\n",
    "                  'By Jove , my quick study of lexicography won a prize .',\n",
    "                  'This is a short sentence .']\n",
    "\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input: {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input: [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input: [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input: [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input: {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    \n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = \\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    \n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\enzi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\enzi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 110288 samples, validate on 27572 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 77s 698us/sample - loss: 3.5096 - acc: 0.4149 - val_loss: 2.5590 - val_acc: 0.4593\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 69s 630us/sample - loss: 2.4364 - acc: 0.4697 - val_loss: 2.3108 - val_acc: 0.4838\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 67s 609us/sample - loss: 2.1916 - acc: 0.5092 - val_loss: 2.0631 - val_acc: 0.5237\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 82s 742us/sample - loss: 1.9536 - acc: 0.5468 - val_loss: 1.8576 - val_acc: 0.5687\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 96s 871us/sample - loss: 1.7942 - acc: 0.5742 - val_loss: 1.7376 - val_acc: 0.5787\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 94s 853us/sample - loss: 1.6968 - acc: 0.5809 - val_loss: 1.6596 - val_acc: 0.5883\n",
      "Epoch 7/10\n",
      " 14336/110288 [==>...........................] - ETA: 1:17 - loss: 1.6593 - acc: 0.5856"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    input_seq = Input(input_shape[1:])\n",
    "    rnn = GRU(64, return_sequences=True)(input_seq)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
    "    model = Model(input_seq, Activation('softmax')(logits))\n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tests.test_simple_model(simple_model)\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length, \n",
    "    english_vocab_size+1, \n",
    "    french_vocab_size+1)\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    rnn = GRU(64, return_sequences=True, activation=\"tanh\")\n",
    "    \n",
    "    embedding = Embedding(french_vocab_size, 64, input_length=input_shape[1])\n",
    "    logits = TimeDistributed(Dense(french_vocab_size, activation=\"softmax\"))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(rnn)\n",
    "    model.add(logits)\n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "#tests.test_embed_model(embed_model)\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "embeded_model = embed_model(tmp_x.shape, max_french_sequence_length, english_vocab_size+1, french_vocab_size+1)\n",
    "\n",
    "embeded_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "print(logits_to_text(embeded_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27572 samples\n",
      "Epoch 1/20\n",
      "110288/110288 [==============================] - 220s 2ms/sample - loss: 2.7350 - acc: 0.4954 - val_loss: 1.8011 - val_acc: 0.5739\n",
      "Epoch 2/20\n",
      "110288/110288 [==============================] - 226s 2ms/sample - loss: 1.6648 - acc: 0.5848 - val_loss: 1.4867 - val_acc: 0.6076\n",
      "Epoch 3/20\n",
      "110288/110288 [==============================] - 242s 2ms/sample - loss: 1.4590 - acc: 0.6114 - val_loss: 1.3664 - val_acc: 0.6190\n",
      "Epoch 4/20\n",
      "110288/110288 [==============================] - 211s 2ms/sample - loss: 1.3509 - acc: 0.6279 - val_loss: 1.3006 - val_acc: 0.6338\n",
      "Epoch 5/20\n",
      "110288/110288 [==============================] - 208s 2ms/sample - loss: 1.2779 - acc: 0.6404 - val_loss: 1.2557 - val_acc: 0.6369\n",
      "Epoch 6/20\n",
      "110288/110288 [==============================] - 184s 2ms/sample - loss: 1.2249 - acc: 0.6512 - val_loss: 1.2346 - val_acc: 0.6367\n",
      "Epoch 7/20\n",
      "110288/110288 [==============================] - 182s 2ms/sample - loss: 1.1809 - acc: 0.6602 - val_loss: 1.2266 - val_acc: 0.6365\n",
      "Epoch 8/20\n",
      "110288/110288 [==============================] - 184s 2ms/sample - loss: 1.1446 - acc: 0.6680 - val_loss: 1.2331 - val_acc: 0.6353\n",
      "Epoch 9/20\n",
      "110288/110288 [==============================] - 178s 2ms/sample - loss: 1.1135 - acc: 0.6743 - val_loss: 1.2303 - val_acc: 0.6336\n",
      "Epoch 10/20\n",
      "110288/110288 [==============================] - 247s 2ms/sample - loss: 1.0880 - acc: 0.6788 - val_loss: 1.2314 - val_acc: 0.6319\n",
      "Epoch 11/20\n",
      "110288/110288 [==============================] - 240s 2ms/sample - loss: 1.0620 - acc: 0.6826 - val_loss: 1.2707 - val_acc: 0.6218\n",
      "Epoch 12/20\n",
      "110288/110288 [==============================] - 253s 2ms/sample - loss: 1.0380 - acc: 0.6858 - val_loss: 1.3008 - val_acc: 0.6205\n",
      "Epoch 13/20\n",
      "110288/110288 [==============================] - 246s 2ms/sample - loss: 1.0169 - acc: 0.6887 - val_loss: 1.3254 - val_acc: 0.6207\n",
      "Epoch 14/20\n",
      "110288/110288 [==============================] - 229s 2ms/sample - loss: 0.9979 - acc: 0.6920 - val_loss: 1.3420 - val_acc: 0.6180\n",
      "Epoch 15/20\n",
      "110288/110288 [==============================] - 195s 2ms/sample - loss: 0.9807 - acc: 0.6943 - val_loss: 1.3875 - val_acc: 0.6162\n",
      "Epoch 16/20\n",
      "110288/110288 [==============================] - 193s 2ms/sample - loss: 0.9661 - acc: 0.6966 - val_loss: 1.4166 - val_acc: 0.6085\n",
      "Epoch 17/20\n",
      "110288/110288 [==============================] - 196s 2ms/sample - loss: 0.9525 - acc: 0.6984 - val_loss: 1.4899 - val_acc: 0.6043\n",
      "Epoch 18/20\n",
      "110288/110288 [==============================] - 178s 2ms/sample - loss: 0.9410 - acc: 0.7005 - val_loss: 1.4696 - val_acc: 0.6062\n",
      "Epoch 19/20\n",
      "110288/110288 [==============================] - 190s 2ms/sample - loss: 0.9277 - acc: 0.7026 - val_loss: 1.5245 - val_acc: 0.6012\n",
      "Epoch 20/20\n",
      "110288/110288 [==============================] - 180s 2ms/sample - loss: 0.9161 - acc: 0.7047 - val_loss: 1.5732 - val_acc: 0.5972\n",
      "new jersey est parfois occupé en printemps mais il est agréable en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True, dropout=0.1), input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_bd_model(bd_model)\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "bidi_model = bd_model(tmp_x.shape, preproc_french_sentences.shape[1], len(english_tokenizer.word_index)+1, \n",
    "                      len(french_tokenizer.word_index)+1)\n",
    "\n",
    "bidi_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "\n",
    "print(logits_to_text(bidi_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27572 samples\n",
      "Epoch 1/20\n",
      "110288/110288 [==============================] - 168s 2ms/sample - loss: 2.9773 - acc: 0.4427 - val_loss: 2.4525 - val_acc: 0.4849\n",
      "Epoch 2/20\n",
      "110288/110288 [==============================] - 188s 2ms/sample - loss: 2.2846 - acc: 0.4989 - val_loss: 2.1141 - val_acc: 0.5046\n",
      "Epoch 3/20\n",
      "110288/110288 [==============================] - 199s 2ms/sample - loss: 2.0115 - acc: 0.5119 - val_loss: 1.9279 - val_acc: 0.5199\n",
      "Epoch 4/20\n",
      "110288/110288 [==============================] - 197s 2ms/sample - loss: 1.8617 - acc: 0.5373 - val_loss: 1.7930 - val_acc: 0.5530\n",
      "Epoch 5/20\n",
      "110288/110288 [==============================] - 180s 2ms/sample - loss: 1.7394 - acc: 0.5623 - val_loss: 1.6855 - val_acc: 0.5697\n",
      "Epoch 6/20\n",
      "110288/110288 [==============================] - 187s 2ms/sample - loss: 1.6423 - acc: 0.5730 - val_loss: 1.5950 - val_acc: 0.5802\n",
      "Epoch 7/20\n",
      "110288/110288 [==============================] - 196s 2ms/sample - loss: 1.5631 - acc: 0.5835 - val_loss: 1.5413 - val_acc: 0.5904\n",
      "Epoch 8/20\n",
      "110288/110288 [==============================] - 174s 2ms/sample - loss: 1.5000 - acc: 0.5953 - val_loss: 1.4762 - val_acc: 0.6023\n",
      "Epoch 9/20\n",
      "110288/110288 [==============================] - 160s 1ms/sample - loss: 1.4500 - acc: 0.6068 - val_loss: 1.4260 - val_acc: 0.6117\n",
      "Epoch 10/20\n",
      "110288/110288 [==============================] - 163s 1ms/sample - loss: 1.4087 - acc: 0.6162 - val_loss: 1.3871 - val_acc: 0.6226\n",
      "Epoch 11/20\n",
      "110288/110288 [==============================] - 169s 2ms/sample - loss: 1.3762 - acc: 0.6235 - val_loss: 1.3535 - val_acc: 0.6286\n",
      "Epoch 12/20\n",
      "110288/110288 [==============================] - 160s 1ms/sample - loss: 1.3409 - acc: 0.6305 - val_loss: 1.3229 - val_acc: 0.6332\n",
      "Epoch 13/20\n",
      "110288/110288 [==============================] - 155s 1ms/sample - loss: 1.3163 - acc: 0.6335 - val_loss: 1.2975 - val_acc: 0.6372\n",
      "Epoch 14/20\n",
      "110288/110288 [==============================] - 168s 2ms/sample - loss: 1.2934 - acc: 0.6365 - val_loss: 1.2776 - val_acc: 0.6402\n",
      "Epoch 15/20\n",
      "110288/110288 [==============================] - 207s 2ms/sample - loss: 1.2691 - acc: 0.6395 - val_loss: 1.2576 - val_acc: 0.6409\n",
      "Epoch 16/20\n",
      "110288/110288 [==============================] - 205s 2ms/sample - loss: 1.2525 - acc: 0.6411 - val_loss: 1.2391 - val_acc: 0.6427\n",
      "Epoch 17/20\n",
      "110288/110288 [==============================] - 180s 2ms/sample - loss: 1.2359 - acc: 0.6443 - val_loss: 1.2184 - val_acc: 0.6504\n",
      "Epoch 18/20\n",
      "110288/110288 [==============================] - 246s 2ms/sample - loss: 1.2199 - acc: 0.6481 - val_loss: 1.1926 - val_acc: 0.6547\n",
      "Epoch 19/20\n",
      "110288/110288 [==============================] - 253s 2ms/sample - loss: 1.1895 - acc: 0.6558 - val_loss: 1.1694 - val_acc: 0.6626\n",
      "Epoch 20/20\n",
      "110288/110288 [==============================] - 156s 1ms/sample - loss: 1.1740 - acc: 0.6592 - val_loss: 1.1521 - val_acc: 0.6653\n",
      "new jersey est jamais agréable en mois et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \n",
    "    learning_rate =1e-3\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape=input_shape[1:], return_sequences=False))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(GRU(128, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_encdec_model(encdec_model)\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[1], 1))\n",
    "\n",
    "encodeco_model = encdec_model(tmp_x.shape, preproc_french_sentences.shape[1], len(english_tokenizer.word_index)+1,\n",
    "                              len(french_tokenizer.word_index)+1)\n",
    "\n",
    "encodeco_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "\n",
    "print(logits_to_text(encodeco_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=english_vocab_size, output_dim=128, input_length=input_shape[1]))\n",
    "    model.add(Bidirectional(GRU(256, return_sequences=False)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    learning_rate = 0.005\n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#tests.test_model_final(model_final)\n",
    "\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27572 samples\n",
      "Epoch 1/17\n",
      " 72704/110288 [==================>...........] - ETA: 3:59 - loss: 2.3765 - acc: 0.4842"
     ]
    }
   ],
   "source": [
    "def final_prediction(x, y, x_tk, y_tk):\n",
    "    \n",
    "    tmp_X = pad(preproc_english_sentences)\n",
    "    model = model_final(tmp_X.shape, preproc_french_sentences.shape[1], len(english_tokenizer.word_index)+1, \n",
    "                       len(french_tokenizer.word_index)+1)\n",
    "    model.fit(tmp_X, preproc_french_sentences, batch_size=1024, epochs=17, validation_split=0.2)\n",
    "    \n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "    return model\n",
    "\n",
    "model = final_prediction(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)\n",
    "\n",
    "y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
    "y_id_to_word[0] = '<PAD>'\n",
    "# TO DO  implent return here\n",
    "sentence = 'he saw a old yellow truck'\n",
    "sentence = [english_tokenizer.word_index[word] for word in sentence.split()]\n",
    "sentence = pad_sequences([sentence], maxlen=preproc_english_sentences.shape[-1], padding='post')\n",
    "sentences = np.array([sentence[0], preproc_english_sentences[0]])\n",
    "predicitions = model.predict(sentences, len(sentences))\n",
    "    \n",
    "print('Sample 1:')\n",
    "print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "print('Il a vu un vieux camion jaune')\n",
    "print('Sample 2:')\n",
    "print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "print(' '.join([y_id_to_word[np.max(x)] for x in preproc_french_sentences[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Susan li : Neural Machine Translation with Python (https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
